{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a73ea8ea-2398-406a-8a81-df7c227590dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é = é : True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from transformers import pipeline\n",
    "from unicodedata import normalize\n",
    "\n",
    "norm_eaccent = normalize('NFKC', '\\u00E9')\n",
    "norm_e_accent = normalize('NFKC', '\\u0065\\u0301')\n",
    "print(f'{norm_eaccent} = {norm_e_accent} : {norm_eaccent == norm_e_accent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dcbe5e6-9b61-46ab-9971-c7003c3646aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é = é : False\n"
     ]
    }
   ],
   "source": [
    "eaccent = '\\u00E9'\n",
    "e_accent = '\\u0065\\u0301'\n",
    "print(f'{eaccent} = {e_accent} : {eaccent == e_accent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21641960-2bcb-4609-b8e6-c742721d40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#byte-pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162510d3-77b0-4fc3-8452-c95b894dc9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SentencePiece Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eccd9cc8-a641-44b1-8267-442bbb715b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NFKC Normalization Form Compatibility Composition\n",
    "def get_hex_encoding(s):\n",
    "    return ' '.join(hex(ord(c)) for c in s)\n",
    "\n",
    "def print_string_and_encoding(s):\n",
    "    print(f'{s} : {get_hex_encoding(s)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a266ac-dfa1-435f-8f54-66f61b210732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é : 0xe9\n",
      "é : 0x65 0x301\n",
      "é : 0xe9\n",
      "é : 0xe9\n"
     ]
    }
   ],
   "source": [
    "for s in [eaccent, e_accent, norm_eaccent, norm_e_accent]:\n",
    "    print_string_and_encoding(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "735e716d-63e7-41fa-af1e-ec1965c0a6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n",
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n",
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x2581 0x69 0x73 0x2581 0x68 0x61 0x72 0x64 0x2e\n"
     ]
    }
   ],
   "source": [
    "#SentencePiece replaces white space with _ (U+2581)\n",
    "s = 'Tokenization is hard.'\n",
    "sn = normalize('NFKC', s)\n",
    "sn_ = sn.replace(' ', '\\u2581')\n",
    "print(get_hex_encoding(s))\n",
    "print(get_hex_encoding(sn))\n",
    "print(get_hex_encoding(sn_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ff73a-ecd1-4438-b974-2da93dbe5079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BPE Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04f57e73-ee2c-468e-91f1-e482cdff4e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing our Data\n",
    "import ast\n",
    "\n",
    "def convert_json_examples_to_text(filepath):\n",
    "    example_jsons = list(map(ast.literal_eval, open(filepath))) # Read in the json from the example file\n",
    "    texts = [example_json['text'].decode('utf-8') for example_json in example_jsons] # Decode the byte sequences\n",
    "    text = '\\n\\n'.join(texts)       # Separate different articles by two newlines\n",
    "    text = normalize('NFKC', text)  # Normalize the text\n",
    "\n",
    "    with open('example.txt', 'w') as fw:\n",
    "        fw.write(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e83ead33-829d-4c61-8986-fa35dc22de96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginners BBQ Class Taking Place in Missoula!\n",
      "Do you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\n",
      "He will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\n",
      "The cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.\n",
      "\n",
      "Discussion in 'Mac OS X Lion (10.7)' started by axboi87, Jan 20, 2012.\n",
      "I've got a 500gb internal drive and a 240gb SSD.\n",
      "When trying to restore using di\n"
     ]
    }
   ],
   "source": [
    "text = convert_json_examples_to_text('./data/data.txt')\n",
    "print(text[:900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf9a7a8-eacc-4994-967f-a10bdef2c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab variable is actually a frequency dictionary of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294cd7c-d504-407f-8a47-945acb347013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#words have been prepended with an underscore to indicate that they are the beginning of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88d79cb8-8a62-4105-a63b-b107493583ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter(['\\u2581' + word for word in text.split()])\n",
    "vocab = {' '.join([l for l in word]): freq for word, freq in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1a656b6-1cef-478d-bee9-75d6ae99ab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_vocab(vocab, end='\\n', limit=20):\n",
    "    \"\"\"Show word frequencys in vocab up to the limit number of words\"\"\"\n",
    "    shown = 0\n",
    "    for word, freq in vocab.items():\n",
    "        print(f'{word}: {freq}', end=end)\n",
    "        shown +=1\n",
    "        if shown > limit:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38288533-7f58-47b8-b783-2f507aa99e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁ B e g i n n e r s: 1\n",
      "▁ B B Q: 3\n",
      "▁ C l a s s: 2\n",
      "▁ T a k i n g: 1\n",
      "▁ P l a c e: 1\n",
      "▁ i n: 15\n",
      "▁ M i s s o u l a !: 1\n",
      "▁ D o: 1\n",
      "▁ y o u: 13\n",
      "▁ w a n t: 1\n",
      "▁ t o: 33\n",
      "▁ g e t: 2\n",
      "▁ b e t t e r: 2\n",
      "▁ a t: 1\n",
      "▁ m a k i n g: 2\n",
      "▁ d e l i c i o u s: 1\n",
      "▁ B B Q ?: 1\n",
      "▁ Y o u: 1\n",
      "▁ w i l l: 6\n",
      "▁ h a v e: 4\n",
      "▁ t h e: 31\n"
     ]
    }
   ],
   "source": [
    "show_vocab(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b97a5864-c42f-481b-a23c-6c19b9a2e7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 455\n",
      "Number of merges required to reproduce SentencePiece training on the whole corpus: 273\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of unique words: {len(vocab)}')\n",
    "print(f'Number of merges required to reproduce SentencePiece training on the whole corpus: {int(0.60*len(vocab))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1174f84f-1e6f-450c-8f61-5f857f6928b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BPE Algorithm\n",
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def get_sentence_piece_vocab(vocab, frac_merges=0.60):\n",
    "    sp_vocab = vocab.copy()\n",
    "    num_merges = int(len(sp_vocab)*frac_merges)\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(sp_vocab)\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        sp_vocab = merge_vocab(best, sp_vocab)\n",
    "\n",
    "    return sp_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e4ce4e4-6d1b-496e-8f6e-161b88b5e8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁B e g in n ers: 1\n",
      "▁BBQ: 3\n",
      "▁Cl ass: 2\n",
      "▁T ak ing: 1\n",
      "▁P la ce: 1\n",
      "▁in: 15\n",
      "▁M is s ou la !: 1\n",
      "▁D o: 1\n",
      "▁you: 13\n",
      "▁w an t: 1\n",
      "▁to: 33\n",
      "▁g et: 2\n",
      "▁be t ter: 2\n",
      "▁a t: 1\n",
      "▁mak ing: 2\n",
      "▁d e l ic i ou s: 1\n",
      "▁BBQ ?: 1\n",
      "▁ Y ou: 1\n",
      "▁will: 6\n",
      "▁have: 4\n",
      "▁the: 31\n"
     ]
    }
   ],
   "source": [
    "sp_vocab = get_sentence_piece_vocab(vocab)\n",
    "show_vocab(sp_vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80bcf9-5c2a-47c6-afa8-5737363de862",
   "metadata": {},
   "outputs": [],
   "source": [
    "########Train SentencePiece BPE Tokenizer on Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "708adefb-e7f7-4a7c-9070-ccb4cbc4fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor(model_file='./data/sentencepiece.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "047fc59d-14a9-4945-86d2-dbfb37382af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Beginn', 'ers', '▁BBQ', '▁Class', '▁', 'Taking', '▁Place', '▁in', '▁Miss', 'oul', 'a', '!']\n",
      "[12847, 277, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 9, 55]\n",
      "Beginners BBQ Class Taking Place in Missoula!\n",
      "Beginners\n"
     ]
    }
   ],
   "source": [
    "s0 = 'Beginners BBQ Class Taking Place in Missoula!'\n",
    "# encode: text => id\n",
    "print(sp.encode_as_pieces(s0))\n",
    "print(sp.encode_as_ids(s0))\n",
    "\n",
    "# decode: id => text\n",
    "print(sp.decode_pieces(sp.encode_as_pieces(s0)))\n",
    "print(sp.decode_ids([12847, 277]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "531a20e7-0cb8-4264-aaca-732888275823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece for ID 15068: ▁BBQ\n",
      "ID for Sentence Piece ▁BBQ: 15068\n",
      "ID for unknown text __MUST_BE_UNKNOWN__: 2\n"
     ]
    }
   ],
   "source": [
    "uid = 15068\n",
    "spiece = \"\\u2581BBQ\"\n",
    "unknown = \"__MUST_BE_UNKNOWN__\"\n",
    "\n",
    "# id <=> piece conversion\n",
    "print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\n",
    "print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\n",
    "\n",
    "# returns 0 for unknown tokens (we can change the id for UNK)\n",
    "print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b83c945-ab96-4f5c-9030-cfc96818a9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning of sentence id: -1\n",
      "Pad id: 0\n",
      "End of sentence id: 1\n",
      "Unknown id: 2\n",
      "Vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "print(f'Beginning of sentence id: {sp.bos_id()}')\n",
    "print(f'Pad id: {sp.pad_id()}')\n",
    "print(f'End of sentence id: {sp.eos_id()}')\n",
    "print(f'Unknown id: {sp.unk_id()}')\n",
    "print(f'Vocab size: {sp.vocab_size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98910d2e-1e14-4a87-a4f7-6a8f2a115426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Id\tSentP\tControl?\n",
      "------------------------\n",
      "0\t<pad>\tTrue\n",
      "1\t</s>\tTrue\n",
      "2\t<unk>\tFalse\n",
      "3\t▁\tFalse\n",
      "4\tX\tFalse\n",
      "5\t.\tFalse\n",
      "6\t,\tFalse\n",
      "7\ts\tFalse\n",
      "8\t▁the\tFalse\n",
      "9\ta\tFalse\n"
     ]
    }
   ],
   "source": [
    "print('\\nId\\tSentP\\tControl?')\n",
    "print('------------------------')\n",
    "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
    "# <s> and </s> are defined as 'control' symbol.\n",
    "for uid in range(10):\n",
    "    print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\n",
    "    \n",
    "# for uid in range(sp.vocab_size()-10,sp.vocab_size()):\n",
    "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7ff2968-1457-470e-801e-ffb8d7142782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** BPE ***\n",
      "['▁B', 'e', 'ginn', 'ers', '▁BBQ', '▁Cl', 'ass', '▁T', 'ak', 'ing', '▁P', 'la', 'ce', '▁in', '▁M', 'is', 's', 'ou', 'la', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=example.txt --model_prefix=example_bpe --vocab_size=450 --model_type=bpe\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: example.txt\n",
      "  input_format: \n",
      "  model_prefix: example_bpe\n",
      "  model_type: BPE\n",
      "  vocab_size: 450\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: example.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 26 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=4533\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9559% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=73\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999559\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 26 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 26\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 455\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=99 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=20 all=732 active=658 piece=▁w\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=40 all=937 active=863 piece=ch\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=60 all=1014 active=940 piece=▁u\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=80 all=1110 active=1036 piece=me\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=100 all=1166 active=1092 piece=la\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=120 all=1217 active=1042 piece=SD\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=140 all=1272 active=1097 piece=▁bu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=160 all=1288 active=1113 piece=▁site\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=180 all=1315 active=1140 piece=ter\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=200 all=1330 active=1155 piece=asure\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=220 all=1339 active=1008 piece=ge\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=240 all=1371 active=1040 piece=▁sh\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=260 all=1384 active=1053 piece=▁cost\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=280 all=1391 active=1060 piece=de\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=300 all=1405 active=1074 piece=000\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=320 all=1427 active=1021 piece=▁GB\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=340 all=1438 active=1032 piece=last\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=360 all=1441 active=1035 piece=▁let\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: example_bpe.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: example_bpe.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=example.txt --model_prefix=example_bpe --vocab_size=450 --model_type=bpe')\n",
    "sp_bpe = spm.SentencePieceProcessor()\n",
    "sp_bpe.load('example_bpe.model')\n",
    "\n",
    "print('*** BPE ***')\n",
    "print(sp_bpe.encode_as_pieces(s0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d955df05-60e1-4c11-af97-b56b0285e915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁B e g in n ers: 1, ▁BBQ: 3, ▁Cl ass: 2, ▁T ak ing: 1, ▁P la ce: 1, ▁in: 15, ▁M is s ou la !: 1, ▁D o: 1, ▁you: 13, ▁w an t: 1, ▁to: 33, ▁g et: 2, ▁be t ter: 2, ▁a t: 1, ▁mak ing: 2, ▁d e l ic i ou s: 1, ▁BBQ ?: 1, ▁ Y ou: 1, ▁will: 6, ▁have: 4, ▁the: 31, "
     ]
    }
   ],
   "source": [
    "show_vocab(sp_vocab, end = ', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c49596f-8b1b-476c-8141-09ffea6b8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "113cf618-18cd-4fda-94c6-b1ac49508a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heapsort(iterable):\n",
    "    h = []\n",
    "    for value in iterable:\n",
    "        heappush(h, value)\n",
    "    return [heappop(h) for i in range(len(h))]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aba2c5ea-6f1b-45c2-afd7-5619d9bd0e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 2, 2, 3, 3, 4, 4]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Optionally try to implement BPE using a priority queue below\n",
    "a = [1,4,3,1,3,2,1,4,2]\n",
    "heapsort(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df527c-768d-4589-828e-cd6f08438f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb812c-6f1a-4eac-baaa-e36d02abe91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question Answering with BERT and HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c00348f-fed6-42da-9aff-b77936240be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipelines\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b749f3af-7e66-4851-9157-9c8ac81432f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b5e4156-3a7b-41af-95f6-0aea578dce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the pipeline for question-answering, which uses the DistilBert model for extractive question answering\n",
    "\n",
    "# The task \"question-answering\" will return a QuestionAnsweringPipeline object\n",
    "question_answerer = pipeline(task=\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699200a-3b16-4a10-b688-d43c063047ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The pipeline question_answerer you just created needs you to pass the question and context as strings.\n",
    "#It returns an answer to the question from the context you provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41a8d1db-c4b5-42e6-a4eb-862941468039",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Tea is an aromatic beverage prepared by pouring hot or boiling water over cured or fresh leaves of Camellia sinensis,\n",
    "an evergreen shrub native to China and East Asia. After water, it is the most widely consumed drink in the world.\n",
    "There are many different types of tea; some, like Chinese greens and Darjeeling, have a cooling, slightly bitter,\n",
    "and astringent flavour, while others have vastly different profiles that include sweet, nutty, floral, or grassy\n",
    "notes. Tea has a stimulating effect in humans primarily due to its caffeine content.\n",
    "\n",
    "The tea plant originated in the region encompassing today's Southwest China, Tibet, north Myanmar and Northeast India,\n",
    "where it was used as a medicinal drink by various ethnic groups. An early credible record of tea drinking dates to\n",
    "the 3rd century AD, in a medical text written by Hua Tuo. It was popularised as a recreational drink during the\n",
    "Chinese Tang dynasty, and tea drinking spread to other East Asian countries. Portuguese priests and merchants\n",
    "introduced it to Europe during the 16th century. During the 17th century, drinking tea became fashionable among the\n",
    "English, who started to plant tea on a large scale in India.\n",
    "\n",
    "The term herbal tea refers to drinks not made from Camellia sinensis: infusions of fruit, leaves, or other plant\n",
    "parts, such as steeps of rosehip, chamomile, or rooibos. These may be called tisanes or herbal infusions to prevent\n",
    "confusion with 'tea' made from the tea plant.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "891e2bc7-8122-4903-ab32-faa395362676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China and East Asia\n"
     ]
    }
   ],
   "source": [
    "result = question_answerer(question=\"Where is tea native to?\", context=context)\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c9c93da-a1d8-46f0-b994-ffec7851e1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where is tea native to? \n",
      ">> China and East Asia\n",
      "When was tea discovered? \n",
      ">> 3rd century AD\n",
      "What is the species name for tea? \n",
      ">> Camellia sinensis\n"
     ]
    }
   ],
   "source": [
    "questions = [\"Where is tea native to?\",\n",
    "             \"When was tea discovered?\",\n",
    "             \"What is the species name for tea?\"]\n",
    "\n",
    "results = question_answerer(question=questions, context=context)\n",
    "\n",
    "for q, r in zip(questions, results):\n",
    "    print(f\"{q} \\n>> {r['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399833dc-8d6d-4336-82f4-d3c63b852962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sometimes you will have particular examples where they don't perform so well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04b6a829-fae2-4490-9269-42d09abcf4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "The Golden Age of Comic Books describes an era of American comic books from the\n",
    "late 1930s to circa 1950. During this time, modern comic books were first published\n",
    "and rapidly increased in popularity. The superhero archetype was created and many\n",
    "well-known characters were introduced, including Superman, Batman, Captain Marvel\n",
    "(later known as SHAZAM!), Captain America, and Wonder Woman.\n",
    "Between 1939 and 1941 Detective Comics and its sister company, All-American Publications,\n",
    "introduced popular superheroes such as Batman and Robin, Wonder Woman, the Flash,\n",
    "Green Lantern, Doctor Fate, the Atom, Hawkman, Green Arrow and Aquaman.[7] Timely Comics,\n",
    "the 1940s predecessor of Marvel Comics, had million-selling titles featuring the Human Torch,\n",
    "the Sub-Mariner, and Captain America.[8]\n",
    "As comic books grew in popularity, publishers began launching titles that expanded\n",
    "into a variety of genres. Dell Comics' non-superhero characters (particularly the\n",
    "licensed Walt Disney animated-character comics) outsold the superhero comics of the day.[12]\n",
    "The publisher featured licensed movie and literary characters such as Mickey Mouse, Donald Duck,\n",
    "Roy Rogers and Tarzan.[13] It was during this era that noted Donald Duck writer-artist\n",
    "Carl Barks rose to prominence.[14] Additionally, MLJ's introduction of Archie Andrews\n",
    "in Pep Comics #22 (December 1941) gave rise to teen humor comics,[15] with the Archie\n",
    "Andrews character remaining in print well into the 21st century.[16]\n",
    "At the same time in Canada, American comic books were prohibited importation under\n",
    "the War Exchange Conservation Act[17] which restricted the importation of non-essential\n",
    "goods. As a result, a domestic publishing industry flourished during the duration\n",
    "of the war which were collectively informally called the Canadian Whites.\n",
    "The educational comic book Dagwood Splits the Atom used characters from the comic\n",
    "strip Blondie.[18] According to historian Michael A. Amundson, appealing comic-book\n",
    "characters helped ease young readers' fear of nuclear war and neutralize anxiety\n",
    "about the questions posed by atomic power.[19] It was during this period that long-running\n",
    "humor comics debuted, including EC's Mad and Carl Barks' Uncle Scrooge in Dell's Four\n",
    "Color Comics (both in 1952).[20][21]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "040094cb-d3c8-4687-b13c-4601459abb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teen humor comics\n"
     ]
    }
   ],
   "source": [
    "question = \"What popular superheroes were introduced between 1939 and 1941?\"\n",
    "\n",
    "result = question_answerer(question=question, context=context)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145bbd4-ea53-422f-b587-12e9d56bf968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, the answer should be: \n",
    "#\"Batman and Robin, Wonder Woman, the Flash, Green Lantern, Doctor Fate, the Atom, Hawkman, Green Arrow, and Aquaman\". \n",
    "#Instead, the pipeline returned a different answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "398f114a-4857-4226-bb25-f558bf7bdfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What popular superheroes were introduced between 1939 and 1941? \n",
      ">> teen humor comics\n",
      "What superheroes were introduced between 1939 and 1941 by Detective Comics and its sister company? \n",
      ">> Archie Andrews\n",
      "What comic book characters were created between 1939 and 1941? \n",
      ">> Archie\n",
      "Andrews\n",
      "What well-known characters were created between 1939 and 1941? \n",
      ">> Archie\n",
      "Andrews\n",
      "What well-known superheroes were introduced between 1939 and 1941 by Detective Comics? \n",
      ">> Archie Andrews\n"
     ]
    }
   ],
   "source": [
    "questions = [\"What popular superheroes were introduced between 1939 and 1941?\",\n",
    "             \"What superheroes were introduced between 1939 and 1941 by Detective Comics and its sister company?\",\n",
    "             \"What comic book characters were created between 1939 and 1941?\",\n",
    "             \"What well-known characters were created between 1939 and 1941?\",\n",
    "             \"What well-known superheroes were introduced between 1939 and 1941 by Detective Comics?\"]\n",
    "\n",
    "results = question_answerer(question=questions, context=context)\n",
    "\n",
    "for q, r in zip(questions, results):\n",
    "    print(f\"{q} \\n>> {r['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc305ab-ca64-4ae9-a1f9-82fc5fee3526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine-tune the DistilBert model using the TyDi QA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cccb8c8-294b-491a-ba5e-c5b48946c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b958ff3c-9705-4831-8113-747390d69bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['passage_answer_candidates', 'question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url'],\n",
       "        num_rows: 9211\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['passage_answer_candidates', 'question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url'],\n",
       "        num_rows: 1031\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The path where the dataset is stored\n",
    "path = './tydiqa_data/'\n",
    "\n",
    "#Load Dataset\n",
    "tydiqa_data = load_from_disk(path)\n",
    "\n",
    "tydiqa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de2f4fff-3ed2-4d7d-8fa2-8475ef19853f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking the object type for one of the elements in the dataset\n",
    "type(tydiqa_data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3ca4ac8-d14a-4924-867e-0d9e4613945c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['passage_answer_candidates', 'question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url'],\n",
       "    num_rows: 9211\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the structure of the dataset\n",
    "tydiqa_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b87c98-f24d-461a-b7a3-a0d56e4ec32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see that each example is like a dictionary object\n",
    "#dataset consists of questions, contexts, and indices that point to the start and end position of the answer inside the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c175d709-7fc3-4356-a58c-90da8cce1c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What mental effects can a mother experience after childbirth?\n",
      "\n",
      "Context (truncated): \n",
      "\n",
      "Postpartum depression (PPD), also called postnatal depression, is a type of mood disorder associated with childbirth, which can affect both sexes.[1][3] Symptoms may include extreme sadness, low energy, anxiety, crying episodes, irritability, and changes in sleeping or eating patterns.[1] Onset is typically between one week and one month following childbirth.[1] PPD can also negatively affect the newborn child.[2]\n",
      "\n",
      "While the exact cause of PPD is unclear, the cause is believed to be a combination of physi ...\n",
      "\n",
      "Answer: Postpartum depression (PPD)\n"
     ]
    }
   ],
   "source": [
    "idx = 600\n",
    "\n",
    "# start index\n",
    "start_index = tydiqa_data['train'][idx]['annotations']['minimal_answers_start_byte'][0]\n",
    "\n",
    "# end index\n",
    "end_index = tydiqa_data['train'][idx]['annotations']['minimal_answers_end_byte'][0]\n",
    "\n",
    "print(f\"Question: {tydiqa_data['train'][idx]['question_text']}\")\n",
    "print(f\"\\nContext (truncated): {tydiqa_data['train'][idx]['document_plaintext'][0:512]} ...\")\n",
    "print(f\"\\nAnswer: {tydiqa_data['train'][idx]['document_plaintext'][start_index:end_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd9429b-8a7d-4be8-83cf-2a48aa14dbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The question answering model predicts a start and endpoint in the context to extract as the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37548e08-9e6d-4a2d-9ed4-8545ec85f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To train your model, you need to pass start and endpoints as labels. \n",
    "#So, you need to implement a function that extracts the start and end positions from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e10e0e66-080d-4240-a716-c5356404027c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passage_answer_candidate_index': [-1],\n",
       " 'minimal_answers_start_byte': [-1],\n",
       " 'minimal_answers_end_byte': [-1],\n",
       " 'yes_no_answer': ['NONE']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tydiqa_data['train'][0]['annotations']#the start and end indices for the unanswerable questions are equal to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bad204-5c48-48e8-a4e3-80c66b4261e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the dataset to work with an object with a table structure instead of a dictionary structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3679c87-1b3f-4531-9757-1daf0a3f22d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening the datasets\n",
    "flattened_train_data = tydiqa_data['train'].flatten()\n",
    "flattened_test_data =  tydiqa_data['validation'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2e5e590-aab1-4a38-b9f9-93b175a7c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a subset of the train dataset\n",
    "flattened_train_data = flattened_train_data.select(range(3000))\n",
    "\n",
    "# Selecting a subset of the test dataset\n",
    "flattened_test_data = flattened_test_data.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53318e3d-af2a-4813-9504-50b0ac43b315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When loading a tokenizer with any method, you must pass the model checkpoint that you want to fine-tune. \n",
    "#Here, you are using the'distilbert-base-cased-distilled-squad' checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4008684-4561-4d76-a6ef-d793879de994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AutoTokenizer from the transformers library\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "# Define max length of sequences in the tokenizer\n",
    "tokenizer.model_max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27827cfc-3624-4889-b556-a7be305ee061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.will use the CLS token when is no answer to a question given a context\n",
    "#2.Tokenizers can split a given string into substrings, \n",
    "#you will need to align the start and end indices with the tokens associated with the target answer word.\n",
    "#3.Finally, a tokenizer can truncate a very long sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b32be55-f5e7-4e7c-9cff-439ebfc47a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing samples using the 3 steps described above\n",
    "def process_samples(sample):\n",
    "    tokenized_data = tokenizer(sample['document_plaintext'], sample['question_text'], truncation=\"only_first\", padding=\"max_length\")\n",
    "\n",
    "    input_ids = tokenized_data[\"input_ids\"]\n",
    "\n",
    "    # We will label impossible answers with the index of the CLS token.\n",
    "    cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "    # If no answers are given, set the cls_index as answer.\n",
    "    if sample[\"annotations.minimal_answers_start_byte\"][0] == -1:\n",
    "        start_position = cls_index\n",
    "        end_position = cls_index\n",
    "    else:\n",
    "        # Start/end character index of the answer in the text.\n",
    "        gold_text = sample[\"document_plaintext\"][sample['annotations.minimal_answers_start_byte'][0]:sample['annotations.minimal_answers_end_byte'][0]]\n",
    "        start_char = sample[\"annotations.minimal_answers_start_byte\"][0]\n",
    "        end_char = sample['annotations.minimal_answers_end_byte'][0] #start_char + len(gold_text)\n",
    "\n",
    "        # sometimes answers are off by a character or two – fix this\n",
    "        if sample['document_plaintext'][start_char-1:end_char-1] == gold_text:\n",
    "            start_char = start_char - 1\n",
    "            end_char = end_char - 1     # When the gold label is off by one character\n",
    "        elif sample['document_plaintext'][start_char-2:end_char-2] == gold_text:\n",
    "            start_char = start_char - 2\n",
    "            end_char = end_char - 2     # When the gold label is off by two characters\n",
    "\n",
    "        start_token = tokenized_data.char_to_token(start_char)\n",
    "        end_token = tokenized_data.char_to_token(end_char - 1)\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_token is None:\n",
    "            start_token = tokenizer.model_max_length\n",
    "        if end_token is None:\n",
    "            end_token = tokenizer.model_max_length\n",
    "\n",
    "        start_position = start_token\n",
    "        end_position = end_token\n",
    "\n",
    "    return {'input_ids': tokenized_data['input_ids'],\n",
    "          'attention_mask': tokenized_data['attention_mask'],\n",
    "          'start_positions': start_position,\n",
    "          'end_positions': end_position}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef298273-f0b1-4daf-a7d2-7855af41cdd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7136556e2d3f4e98a8bd95544076cdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizing and processing the flattened dataset\n",
    "processed_train_data = flattened_train_data.map(process_samples)\n",
    "processed_test_data = flattened_test_data.map(process_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1286b7-15cb-4bbb-b21a-14a567a8a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "754b55c1-980f-4cec-8e9c-62e5c297109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AutoModelForQuestionAnswering for the pre-trained model. You will only fine tune the head of the model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16508fd7-2deb-447c-8a84-cc8b9d2e7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the necessary columns from the datasets to train/test and return them as Pytorch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d4e245a-8e1b-479e-9853-a2b166da3d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_return = ['input_ids','attention_mask', 'start_positions', 'end_positions']\n",
    "\n",
    "processed_train_data.set_format(type='pt', columns=columns_to_return)\n",
    "processed_test_data.set_format(type='pt', columns=columns_to_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8637cb03-fb1d-4d95-b7b2-cf10797babd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score as a metric to evaluate your model's performance.\n",
    "def compute_f1_metrics(pred):\n",
    "    start_labels = pred.label_ids[0]\n",
    "    start_preds = pred.predictions[0].argmax(-1)\n",
    "    end_labels = pred.label_ids[1]\n",
    "    end_preds = pred.predictions[1].argmax(-1)\n",
    "\n",
    "    f1_start = f1_score(start_labels, start_preds, average='macro')\n",
    "    f1_end = f1_score(end_labels, end_preds, average='macro')\n",
    "\n",
    "    return {\n",
    "        'f1_start': f1_start,\n",
    "        'f1_end': f1_end,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78048c4d-83f2-495e-acad-d114f9e978a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the Hugging Face Trainer to fine-tune your model.\n",
    "# Training hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='model_results',     # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size per device during training\n",
    "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
    "    warmup_steps=20,                 # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "# Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,                        # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                 # training arguments, defined above\n",
    "    train_dataset=processed_train_data, # training dataset\n",
    "    eval_dataset=processed_test_data,   # evaluation dataset\n",
    "    compute_metrics=compute_f1_metrics\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9f02ae-fbcd-4619-b2cb-5407c4781db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the fine-tuned model's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c39745-320c-4129-bc6e-b9f8aa094400",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(processed_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad5860-2bb3-4f5b-a838-40139a271704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using your Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea59afb-f113-4537-b508-2de35477aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = r\"\"\"\n",
    "The Golden Age of Comic Books describes an era of American comic books from the\n",
    "late 1930s to circa 1950. During this time, modern comic books were first published\n",
    "and rapidly increased in popularity. The superhero archetype was created and many\n",
    "well-known characters were introduced, including Superman, Batman, Captain Marvel\n",
    "(later known as SHAZAM!), Captain America, and Wonder Woman.\n",
    "Between 1939 and 1941 Detective Comics and its sister company, All-American Publications,\n",
    "introduced popular superheroes such as Batman and Robin, Wonder Woman, the Flash,\n",
    "Green Lantern, Doctor Fate, the Atom, Hawkman, Green Arrow and Aquaman.[7] Timely Comics,\n",
    "the 1940s predecessor of Marvel Comics, had million-selling titles featuring the Human Torch,\n",
    "the Sub-Mariner, and Captain America.[8]\n",
    "As comic books grew in popularity, publishers began launching titles that expanded\n",
    "into a variety of genres. Dell Comics' non-superhero characters (particularly the\n",
    "licensed Walt Disney animated-character comics) outsold the superhero comics of the day.[12]\n",
    "The publisher featured licensed movie and literary characters such as Mickey Mouse, Donald Duck,\n",
    "Roy Rogers and Tarzan.[13] It was during this era that noted Donald Duck writer-artist\n",
    "Carl Barks rose to prominence.[14] Additionally, MLJ's introduction of Archie Andrews\n",
    "in Pep Comics #22 (December 1941) gave rise to teen humor comics,[15] with the Archie\n",
    "Andrews character remaining in print well into the 21st century.[16]\n",
    "At the same time in Canada, American comic books were prohibited importation under\n",
    "the War Exchange Conservation Act[17] which restricted the importation of non-essential\n",
    "goods. As a result, a domestic publishing industry flourished during the duration\n",
    "of the war which were collectively informally called the Canadian Whites.\n",
    "The educational comic book Dagwood Splits the Atom used characters from the comic\n",
    "strip Blondie.[18] According to historian Michael A. Amundson, appealing comic-book\n",
    "characters helped ease young readers' fear of nuclear war and neutralize anxiety\n",
    "about the questions posed by atomic power.[19] It was during this period that long-running\n",
    "humor comics debuted, including EC's Mad and Carl Barks' Uncle Scrooge in Dell's Four\n",
    "Color Comics (both in 1952).[20][21]\n",
    "\"\"\"\n",
    "\n",
    "questions = [\"What superheroes were introduced between 1939 and 1941 by Detective Comics and its sister company?\",\n",
    "             \"What comic book characters were created between 1939 and 1941?\",\n",
    "             \"What well-known characters were created between 1939 and 1941?\",\n",
    "             \"What well-known superheroes were introduced between 1939 and 1941 by Detective Comics?\"]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer.encode_plus(question, text, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    inputs.to(\"cuda\")\n",
    "\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer_model = model(**inputs)\n",
    "    \n",
    "    start_logits = answer_model['start_logits'].cpu().detach().numpy()\n",
    "\n",
    "    answer_start = np.argmax(start_logits)  \n",
    "    \n",
    "    end_logits = answer_model['end_logits'].cpu().detach().numpy()\n",
    "    \n",
    "    # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = np.argmax(end_logits) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b938d2-5955-4efe-8a2d-bfb5c14b5d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
