{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "368f1296-8597-49b5-863e-45880633263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji==1.4.1 in /opt/anaconda3/lib/python3.11/site-packages (1.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji==1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb916205-8734-427b-918c-775bc8b49885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/wenqian/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "import emoji\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from utils2 import get_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60dcc18d-9e3c-4f17-926b-bd5c98bf4ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data processing: lowercase;punctuation;numbers/special characters/special words\n",
    "#get windows\n",
    "#find a way to represent the central/context words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf736179-0368-40ef-b12d-ed89a3d4aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a corpus\n",
    "corpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "382294e9-4626-4c0a-aae9-a56b756e020a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  Who ❤️ \"word embeddings\" in 2020? I do!!!\n",
      "After cleaning punctuation:  Who ❤️ \"word embeddings\" in 2020. I do.\n"
     ]
    }
   ],
   "source": [
    "# Print original corpus\n",
    "print(f'Corpus:  {corpus}')\n",
    "\n",
    "# Do the substitution\n",
    "data = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "\n",
    "# Print cleaned corpus\n",
    "print(f'After cleaning punctuation:  {data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15ed13c1-7094-4ea8-8e20-856d82c71f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial string:  Who ❤️ \"word embeddings\" in 2020. I do.\n",
      "After tokenization:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "# Print cleaned corpus\n",
    "print(f'Initial string:  {data}')\n",
    "\n",
    "# Tokenize the cleaned corpus\n",
    "data = nltk.word_tokenize(data)\n",
    "\n",
    "# Print the tokenized version of the corpus\n",
    "print(f'After tokenization:  {data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e0be4c6-ec13-482f-8254-f6691ee42de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of numbers and punctuation other than periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5cc26ac2-4ea4-43ca-bebe-6058f3dcd9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial list of tokens:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n",
      "After cleaning:  ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "# Print the tokenized version of the corpus\n",
    "print(f'Initial list of tokens:  {data}')\n",
    "\n",
    "# Filter tokenized corpus using list comprehension\n",
    "data = [ ch.lower() for ch in data\n",
    "         if ch.isalpha()#if the character ch is alphabetic\n",
    "         or ch == '.'\n",
    "         or emoji.get_emoji_regexp().search(ch)\n",
    "       ]\n",
    "\n",
    "# Print the tokenized and filtered version of the corpus\n",
    "print(f'After cleaning:  {data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "165cc4fc-27a5-454b-8cb8-363c667e9720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'tokenize' function that will include the steps previously seen\n",
    "#lowercase;punctuation;numbers/special characters/special words\n",
    "def tokenize(corpus):\n",
    "    data = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "    data = nltk.word_tokenize(data)  # tokenize string to words\n",
    "    data = [ ch.lower() for ch in data\n",
    "             if ch.isalpha()\n",
    "             or ch == '.'\n",
    "             or emoji.get_emoji_regexp().search(ch)\n",
    "           ]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab2cdc1a-8fd0-4535-ae72-823b4a359896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  I am happy because I am learning\n",
      "Words (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n"
     ]
    }
   ],
   "source": [
    "#apply the function\n",
    "# Define new corpus\n",
    "corpus = 'I am happy because I am learning'\n",
    "\n",
    "# Print new corpus\n",
    "print(f'Corpus:  {corpus}')\n",
    "\n",
    "# Save tokenized version of corpus into 'words' variable\n",
    "words = tokenize(corpus)\n",
    "\n",
    "# Print the tokenized version of the corpus\n",
    "print(f'Words (tokens):  {words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05646e2c-831d-4ed9-b709-cd0d726c8059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['now', 'it', 'your', 'turn', 'try', 'with', 'your', 'own', 'sentence', '.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this with any sentence\n",
    "tokenize(\"Now it's your turn: try with your own sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6319f294-cbd3-4a18-a1c9-7b583ff6f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sliding window of words\n",
    "# Define the 'get_windows' function\n",
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "        yield context_words, center_word\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7efcf0a0-222b-4b2d-b819-c71f9b5551b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'because', 'i']\thappy\n",
      "['am', 'happy', 'i', 'am']\tbecause\n",
      "['happy', 'because', 'am', 'learning']\ti\n"
     ]
    }
   ],
   "source": [
    "# Print 'context_words' and 'center_word' for the new corpus with a 'context half-size' of 2\n",
    "for x, y in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n",
    "    print(f'{x}\\t{y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c1dd536c-2a51-463b-aa84-c785aeb1ae14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['now', 'your']\tit\n",
      "['it', 'turn']\tyour\n",
      "['your', 'try']\tturn\n",
      "['turn', 'with']\ttry\n",
      "['try', 'your']\twith\n",
      "['with', 'own']\tyour\n",
      "['your', 'sentence']\town\n",
      "['own', '.']\tsentence\n"
     ]
    }
   ],
   "source": [
    "# Print 'context_words' and 'center_word' for any sentence with a 'context half-size' of 1\n",
    "for x, y in get_windows(tokenize(\"Now it's your turn: try with your own sentence!\"), 1):\n",
    "    print(f'{x}\\t{y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e893695e-47cf-4633-a274-1f86bf06f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming\n",
    "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
    "word2Ind, Ind2word = get_dict(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc6f1b5b-243b-4baf-86f8-d3d11503fc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print 'word2Ind' dictionary\n",
    "word2Ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "384ea64b-3c80-4e31-a8ae-04800c0ec7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'i':   3\n"
     ]
    }
   ],
   "source": [
    "# Print value for the key 'i' within word2Ind dictionary\n",
    "print(\"Index of the word 'i':  \",word2Ind['i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d820f465-e0e9-4bc4-9011-52766381fc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print 'Ind2word' dictionary\n",
    "Ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "113706b1-51f3-4dbc-840d-ff0bb3976fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word which has index 2:   happy\n"
     ]
    }
   ],
   "source": [
    "# Print value for the key '2' within Ind2word dictionary\n",
    "print(\"Word which has index 2:  \",Ind2word[2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e3f8acb-3a53-42a1-b476-82512102f5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5\n"
     ]
    }
   ],
   "source": [
    "# Save length of word2Ind dictionary into the 'V' variable\n",
    "V = len(word2Ind)\n",
    "\n",
    "# Print length of word2Ind dictionary\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "821306f9-64ff-4d68-be46-9b1cae3504df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting one-hot word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "01d28af2-22e4-437f-9218-0bb5bad1cdb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = word2Ind['happy']\n",
    "center_word_vector = np.zeros(V) #v row\n",
    "len(center_word_vector) == V #Assert\n",
    "center_word_vector[n] = 1 #Replace element number 'n' with a 1\n",
    "center_word_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3a9bcba4-013c-4255-81da-99aea61d80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group all of these steps in a convenient function\n",
    "# Define the 'word_to_one_hot_vector' function that will include the steps previously seen\n",
    "def word_to_one_hot_vector(word, word2Ind, V):\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[word2Ind[word]] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2cecf627-910f-4c4f-9abc-f7acf43fc70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_one_hot_vector('happy', word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b698a2aa-bfed-4393-b912-75749e669cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_one_hot_vector('learning', word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e669f64f-1e2e-40dd-ac7d-001c22783893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting context vector \n",
    "#(sum(one_hot_vector))/C\n",
    "#C is the half vector size\n",
    "def context_words_to_vector(context_words, word2Ind, V):\n",
    "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
    "    context_words_vectors = np.mean(context_words_vectors, axis=0)\n",
    "    return context_words_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "08320a04-6d40-4ffe-851b-dff3c17e7f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "51eeeb2d-6aa6-4c22-b81b-ad6cf17b73cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create one-hot vectors for each context word using list comprehension\n",
    "context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
    "\n",
    "# Print one-hot vectors for each context word\n",
    "context_words_vectors\n",
    "# Compute mean of the vectors using numpy\n",
    "np.mean(context_words_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d6e5e7bb-544b-499f-b927-ed1aa2f7e678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'happy', 'because', 'i', 'am', 'learning']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######Building the training set\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "27f9933e-7c05-440a-89af-4edcf3f52356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context words:  ['i', 'am', 'because', 'i'] -> [0.25 0.25 0.   0.5  0.  ]\n",
      "Center word:  happy -> [0. 0. 1. 0. 0.]\n",
      "\n",
      "Context words:  ['am', 'happy', 'i', 'am'] -> [0.5  0.   0.25 0.25 0.  ]\n",
      "Center word:  because -> [0. 1. 0. 0. 0.]\n",
      "\n",
      "Context words:  ['happy', 'because', 'am', 'learning'] -> [0.25 0.25 0.25 0.   0.25]\n",
      "Center word:  i -> [0. 0. 0. 1. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for context_words, center_word in get_windows(words, 2):  # reminder: 2 is the context half-size\n",
    "    print(f'Context words:  {context_words} -> {context_words_to_vector(context_words, word2Ind, V)}')\n",
    "    print(f'Center word:  {center_word} -> {word_to_one_hot_vector(center_word, word2Ind, V)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "857a24ad-9e0b-4805-beb5-1c9fb4ce453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the generator function 'get_training_example'\n",
    "def get_training_example(words, C, word2Ind, V):\n",
    "    for context_words, center_word in get_windows(words, C):\n",
    "        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cd5090de-de44-4139-add9-29db5c976596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context words vector:  [0.25 0.25 0.   0.5  0.  ]\n",
      "Center word vector:  [0. 0. 1. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.5  0.   0.25 0.25 0.  ]\n",
      "Center word vector:  [0. 1. 0. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.25 0.25 0.25 0.   0.25]\n",
      "Center word vector:  [0. 0. 0. 1. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n",
    "    print(f'Context words vector:  {context_words_vector}')\n",
    "    print(f'Center word vector:  {center_word_vector}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49447d0d-a075-4f42-bdfb-961a5728de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############labs 2 CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c989fac0-c644-4881-b061-6c003300df9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.71320643],\n",
       "       [-4.79248051],\n",
       "       [ 1.33648235],\n",
       "       [ 2.48803883],\n",
       "       [-0.01492988]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a random seed so all random outcomes can be reproduced\n",
    "np.random.seed(10)\n",
    "\n",
    "# Define a 5X1 column vector, using numpy random.rand function returns a numpy array filled with values taken from a uniform distribution over [0, 1)\n",
    "z_1 = 10*np.random.rand(5, 1)-5 #Numpy allows vectorization so each value is multiplied by 10 and then substracted 5\n",
    "\n",
    "# Print the vector\n",
    "z_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2ea6f5d3-0f89-49b1-a623-28a8f0067249",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = z_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9e7972a3-34fd-46c3-870d-51d46625bb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8dbb7339-78e9-4735-b99e-a444f0a3a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReLU\n",
    "h[h < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c3792987-7d0d-49d7-8202-57cd4977106d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.71320643],\n",
       "       [0.        ],\n",
       "       [1.33648235],\n",
       "       [2.48803883],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f8ddd787-f1ef-4eca-aad2-424651542c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'relu' function that will include the steps previously seen\n",
    "def relu(z):\n",
    "    result = z.copy()\n",
    "    result[result < 0] = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "67f00421-451a-4396-9417-cedb0e9ea493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [4.50714306],\n",
       "       [2.31993942],\n",
       "       [0.98658484],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a new vector and save it in the 'z' variable\n",
    "z = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\n",
    "\n",
    "# Apply ReLU to it\n",
    "relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6f0bc8cb-0a1e-4ccb-a458-c8adb4c37467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#softmax\n",
    "z = np.array([9, 8, 11, 10, 8.5])\n",
    "\n",
    "# Print the vector\n",
    "np.add(z[1],z[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "79e7381b-2779-4169-8ddd-eee9d02c9ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    numerator = np.exp(z)\n",
    "    denominator = np.sum(numerator)\n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c18f9e7b-39b1-40ac-9bcf-97c558a233a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "945217f6-1360-4d3b-9e6f-8e0dc72ef1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(softmax(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "39709b5f-ebcc-4769-aec7-3400125976b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dimensions\n",
    "# Define V. Remember this was the size of the vocabulary in the previous lecture notebook\n",
    "V = 5\n",
    "\n",
    "# Define vector of length V filled with zeros\n",
    "x_array = np.zeros(V)\n",
    "\n",
    "# Print vector\n",
    "x_array\n",
    "\n",
    "# Print vector's shape\n",
    "x_array.shape #a 1-dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b569aa0a-7384-47f3-9694-edc975d37749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy vector\n",
    "x_column_vector = x_array.copy()\n",
    "\n",
    "# Reshape copy of vector\n",
    "x_column_vector.shape = (V, 1)  # alternatively ... = (x_array.shape[0], 1)\n",
    "\n",
    "# Print vector\n",
    "x_column_vector\n",
    "\n",
    "# Print vector's shape\n",
    "x_column_vector.shape#2-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471f94c4-c7ac-4bde-8841-8315d0050dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########Training the CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4a29c97b-c391-4448-9782-172547f14e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the word embedding vectors and save it in the variable 'N'\n",
    "N = 3\n",
    "\n",
    "# Define V. Remember this was the size of the vocabulary in the previous lecture notebooks\n",
    "V = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "661433e0-9535-4a9f-a6a2-ff1ea1a92154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define first matrix of weights\n",
    "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
    "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
    "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
    "\n",
    "# Define second matrix of weights\n",
    "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
    "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
    "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
    "               [ 0.07055222, -0.02015138,  0.36107434],\n",
    "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
    "\n",
    "# Define first vector of biases\n",
    "b1 = np.array([[ 0.09688219],\n",
    "               [ 0.29239497],\n",
    "               [-0.27364426]])\n",
    "\n",
    "# Define second vector of biases\n",
    "b2 = np.array([[ 0.0352008 ],\n",
    "               [-0.36393384],\n",
    "               [-0.12775555],\n",
    "               [-0.34802326],\n",
    "               [-0.07017815]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a0dc6c07-b773-4fc1-86c6-de5ef73777ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (vocabulary size): 5\n",
      "N (embedding size / size of the hidden layer): 3\n",
      "size of W1: (3, 5) (NxV)\n",
      "size of b1: (3, 1) (Nx1)\n",
      "size of W2: (5, 3) (VxN)\n",
      "size of b2: (5, 1) (Vx1)\n"
     ]
    }
   ],
   "source": [
    "print(f'V (vocabulary size): {V}')\n",
    "print(f'N (embedding size / size of the hidden layer): {N}')\n",
    "print(f'size of W1: {W1.shape} (NxV)')\n",
    "print(f'size of b1: {b1.shape} (Nx1)')\n",
    "print(f'size of W2: {W2.shape} (VxN)')\n",
    "print(f'size of b2: {b2.shape} (Vx1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "aaf4c0ed-2ce5-4bb8-bfa3-0f20485dceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenized version of the corpus\n",
    "words = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
    "\n",
    "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
    "word2Ind, Ind2word = get_dict(words)\n",
    "\n",
    "# Define the 'get_windows' function as seen in a previous notebook\n",
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "        yield context_words, center_word\n",
    "        i += 1\n",
    "\n",
    "# Define the 'word_to_one_hot_vector' function as seen in a previous notebook\n",
    "def word_to_one_hot_vector(word, word2Ind, V):\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[word2Ind[word]] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "# Define the 'context_words_to_vector' function as seen in a previous notebook\n",
    "def context_words_to_vector(context_words, word2Ind, V):\n",
    "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
    "    context_words_vectors = np.mean(context_words_vectors, axis=0)\n",
    "    return context_words_vectors\n",
    "\n",
    "# Define the generator function 'get_training_example' as seen in a previous notebook\n",
    "def get_training_example(words, C, word2Ind, V):\n",
    "    for context_words, center_word in get_windows(words, C):\n",
    "        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9e0442ed-7cf6-4012-b0e1-23fad8e8c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generator object in the 'training_examples' variable with the desired arguments\n",
    "training_examples = get_training_example(words, 2, word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6be0963a-f1d1-4e91-8382-124435e6116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first values from generator\n",
    "x_array, y_array = next(training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0ae260f4-c96d-4c63-b41d-fdf2bd293dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print context words vector\n",
    "x_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7e0716e6-5639-476c-ab8e-b73b1551828c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print one hot vector of center word\n",
    "y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f5361aed-0010-4342-b5a7-531193f1569f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "[[0.25]\n",
      " [0.25]\n",
      " [0.  ]\n",
      " [0.5 ]\n",
      " [0.  ]]\n",
      "\n",
      "y:\n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Copy vector\n",
    "x = x_array.copy()\n",
    "\n",
    "# Reshape it\n",
    "x.shape = (V, 1)\n",
    "\n",
    "# Print it\n",
    "print(f'x:\\n{x}\\n')\n",
    "\n",
    "# Copy vector\n",
    "y = y_array.copy()\n",
    "\n",
    "# Reshape it\n",
    "y.shape = (V, 1)\n",
    "\n",
    "# Print it\n",
    "print(f'y:\\n{y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7d251810-75f2-4973-8f96-b537a6640307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'relu' function as seen in the previous lecture notebook\n",
    "def relu(z):\n",
    "    result = z.copy()\n",
    "    result[result < 0] = 0\n",
    "    return result\n",
    "\n",
    "# Define the 'softmax' function as seen in the previous lecture notebook\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    sum_e_z = np.sum(e_z)\n",
    "    return e_z / sum_e_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fa08e5c1-2076-49e9-bb8e-29aa1fa45796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36483875],\n",
       "       [ 0.63710329],\n",
       "       [-0.3236647 ]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute z1 (values of first hidden layer before applying the ReLU function)\n",
    "z1 = np.dot(W1, x) + b1\n",
    "z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "af28f51f-72e9-4ce1-86cf-8fc1b04f2eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36483875],\n",
       "       [0.63710329],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = relu(z1)\n",
    "\n",
    "# Print h\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5d9a60f7-06cc-48b6-8f2d-9a732384ef68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.31973737],\n",
       "       [-0.28125477],\n",
       "       [-0.09838369],\n",
       "       [-0.33512159],\n",
       "       [-0.19919612]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Values of the output layer\n",
    "# Compute z2 (values of the output layer before applying the softmax function)\n",
    "z2 = np.dot(W2, h) + b2\n",
    "\n",
    "# Print z2\n",
    "z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6686b9ef-673c-4693-b151-a312fe226472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18519074],\n",
       "       [0.19245626],\n",
       "       [0.23107446],\n",
       "       [0.18236353],\n",
       "       [0.20891502]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute y_hat (z2 after applying softmax function)\n",
    "y_hat = softmax(z2)\n",
    "\n",
    "# Print y_hat\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "522a177f-c5ab-4113-93e9-c9fb5e895b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross-entropy loss\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d54cd3dd-389d-4f24-a763-5526766bf522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4650152923611106"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -np.sum(np.log(y_hat)*y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "52ea8899-5a93-4987-a836-3845874c7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_predicted, y_actual):\n",
    "    # Fill the loss variable with your code\n",
    "    loss = -np.sum(np.log(y_predicted)*y_actual)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "198163d2-3fc8-4520-bab5-1e4861b58105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4650152923611106"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "58994817-c140-4f74-bfdb-bf18728089f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropagation\n",
    "grad_W1 = np.dot(relu(np.dot(W2.T,y_hat - y)),x.T)\n",
    "grad_W2 = np.dot(y_hat - y, h.T)\n",
    "grad_b1 = relu(np.dot(W2.T,y_hat - y))\n",
    "grad_b2 = y_hat - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "17b4f35f-71a7-47ec-a0d9-8faf351315d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "81e35a17-7f0e-4109-a857-f098ef0e2f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06756476,  0.11798563,  0.        ],\n",
       "       [ 0.0702155 ,  0.12261452,  0.        ],\n",
       "       [-0.28053384, -0.48988499,  0.        ],\n",
       "       [ 0.06653328,  0.1161844 ,  0.        ],\n",
       "       [ 0.07622029,  0.13310045,  0.        ]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8aea325f-b5b6-47b6-91a7-46ea75ab9da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.        ],\n",
       "       [0.17045858]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "48d4212f-fe5e-4db3-a17d-6bec143840a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18519074],\n",
       "       [ 0.19245626],\n",
       "       [-0.76892554],\n",
       "       [ 0.18236353],\n",
       "       [ 0.20891502]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4e4a08d4-2b2f-4222-8241-bedb2b149e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (vocabulary size): 5\n",
      "N (embedding size / size of the hidden layer): 3\n",
      "size of grad_W1: (3, 5) (NxV)\n",
      "size of grad_b1: (3, 1) (Nx1)\n",
      "size of grad_W2: (5, 3) (VxN)\n",
      "size of grad_b2: (5, 1) (Vx1)\n"
     ]
    }
   ],
   "source": [
    "print(f'V (vocabulary size): {V}')\n",
    "print(f'N (embedding size / size of the hidden layer): {N}')\n",
    "print(f'size of grad_W1: {grad_W1.shape} (NxV)')\n",
    "print(f'size of grad_b1: {grad_b1.shape} (Nx1)')\n",
    "print(f'size of grad_W2: {grad_W2.shape} (VxN)')\n",
    "print(f'size of grad_b2: {grad_b2.shape} (Vx1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dbdd61-c96f-4d87-9e8d-b6ac85054480",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fc67774f-4bbd-4768-a766-3e01807462cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define alpha\n",
    "alpha = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d9a09ff1-d0c8-4d6f-b927-4ec3de07ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute updated W1\n",
    "W1_new = W1 - alpha * grad_W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ebe50147-314e-4b99-87ae-32166f61266c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old value of W1:\n",
      "[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n",
      " [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n",
      " [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n",
      "\n",
      "new value of W1:\n",
      "[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n",
      " [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n",
      " [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]\n"
     ]
    }
   ],
   "source": [
    "print('old value of W1:')\n",
    "print(W1)\n",
    "print()\n",
    "print('new value of W1:')\n",
    "print(W1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0695eda6-39f2-4b5c-94a4-e69bc3f5d46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2_new\n",
      "[[-0.22384758 -0.43362588  0.13310965]\n",
      " [ 0.08265956  0.0775535   0.1772054 ]\n",
      " [ 0.19557112 -0.04637608 -0.1790735 ]\n",
      " [ 0.06855622 -0.02363691  0.36107434]\n",
      " [ 0.33251813 -0.3982269  -0.43959196]]\n",
      "\n",
      "b1_new\n",
      "[[ 0.09688219]\n",
      " [ 0.29239497]\n",
      " [-0.27875802]]\n",
      "\n",
      "b2_new\n",
      "[[ 0.02964508]\n",
      " [-0.36970753]\n",
      " [-0.10468778]\n",
      " [-0.35349417]\n",
      " [-0.0764456 ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute updated W2\n",
    "W2_new = W2 - alpha * grad_W2\n",
    "\n",
    "# Compute updated b1\n",
    "b1_new = b1 - alpha * grad_b1\n",
    "\n",
    "# Compute updated b2\n",
    "b2_new = b2 - alpha * grad_b2\n",
    "\n",
    "\n",
    "print('W2_new')\n",
    "print(W2_new)\n",
    "print()\n",
    "print('b1_new')\n",
    "print(b1_new)\n",
    "print()\n",
    "print('b2_new')\n",
    "print(b2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f169da34-7f3d-4c57-b479-d5ce8d1b7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1d11749b-cf0c-4f11-b39d-1ae0dfaada30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenized version of the corpus\n",
    "words = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
    "\n",
    "# Define V. Remember this is the size of the vocabulary\n",
    "V = 5\n",
    "\n",
    "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
    "word2Ind, Ind2word = get_dict(words)\n",
    "\n",
    "\n",
    "# Define first matrix of weights\n",
    "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
    "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
    "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
    "\n",
    "# Define second matrix of weights\n",
    "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
    "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
    "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
    "               [ 0.07055222, -0.02015138,  0.36107434],\n",
    "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
    "\n",
    "# Define first vector of biases\n",
    "b1 = np.array([[ 0.09688219],\n",
    "               [ 0.29239497],\n",
    "               [-0.27364426]])\n",
    "\n",
    "# Define second vector of biases\n",
    "b2 = np.array([[ 0.0352008 ],\n",
    "               [-0.36393384],\n",
    "               [-0.12775555],\n",
    "               [-0.34802326],\n",
    "               [-0.07017815]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fa55e28e-f0f9-4114-ae94-62b388bb89a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am\n",
      "because\n",
      "happy\n",
      "i\n",
      "learning\n"
     ]
    }
   ],
   "source": [
    "# Print corresponding word for each index within vocabulary's range\n",
    "for i in range(V):\n",
    "    print(Ind2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4110d49b-f74c-4469-af1c-245675be681b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [0.41687358 0.32735501 0.26637602]\n",
      "because: [ 0.08854191  0.22795148 -0.23846886]\n",
      "happy: [-0.23495225 -0.23951958 -0.37770863]\n",
      "i: [ 0.28320538  0.4117634  -0.11399446]\n",
      "learning: [ 0.41800106 -0.23924344  0.34008124]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each word of the vocabulary\n",
    "for word in word2Ind:\n",
    "    # Extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W1[:, word2Ind[word]] #whole row, col index\n",
    "    # Print word alongside word embedding vector\n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "98e426a0-b56a-4eb9-9a51-c5ad33fbb36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [-0.22182064 -0.43008631  0.13310965]\n",
      "because: [0.08476603 0.08123194 0.1772054 ]\n",
      "happy: [ 0.1871551  -0.06107263 -0.1790735 ]\n",
      "i: [ 0.07055222 -0.02015138  0.36107434]\n",
      "learning: [ 0.33480474 -0.39423389 -0.43959196]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each word of the vocabulary\n",
    "for word in word2Ind:\n",
    "    # Extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W2.T[:, word2Ind[word]]\n",
    "    # Print word alongside word embedding vector\n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5c05db9d-5152-4084-a7ca-3d7594dcaf13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n",
       "       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n",
       "       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute W3 as the average of W1 and W2 transposed\n",
    "W3 = (W1+W2.T)/2\n",
    "\n",
    "# Print W3\n",
    "W3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "fe60fa19-f98c-485f-836c-9371084e6f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [ 0.09752647 -0.05136565  0.19974284]\n",
      "because: [ 0.08665397  0.15459171 -0.03063173]\n",
      "happy: [-0.02389858 -0.15029611 -0.27839106]\n",
      "i: [0.1768788  0.19580601 0.12353994]\n",
      "learning: [ 0.3764029  -0.31673866 -0.04975536]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each word of the vocabulary\n",
    "for word in word2Ind:\n",
    "    # Extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W3[:, word2Ind[word]]\n",
    "    # Print word alongside word embedding vector\n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
